{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import numpy as np\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "from timeit import default_timer as timer\n",
    "if '__file__' in locals():\n",
    "    dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "else:\n",
    "    dir_path = os.getcwd()\n",
    "python_path = dir_path + '/../python/'\n",
    "sys.path.append(python_path)\n",
    "\n",
    "\n",
    "\n",
    "import dataset_loader\n",
    "importlib.reload(dataset_loader)\n",
    "from dataset_loader import EM_DATA_REAL_SYTH, EM_DATA,EM_DATA_DISC_RANDOM\n",
    "from dataset_loader import BATCH_SIZE, NBOX_IN,NBOX_OUT,N_CHANNELS\n",
    "from dataset_loader import getbox\n",
    "import net_3d_1\n",
    "import utils\n",
    "from utils import get_available_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6nt8 Loaded\n",
      "5fik Loaded\n",
      "/Users/markroza/Documents/work_from_home/NNcourse_project/data//res6/synth_exp//F_5flc_output.npy\n",
      "5flc FAILED, Error : \n",
      "4uer Loaded\n",
      "5vhh Loaded\n",
      "4uif Loaded\n",
      "4uer Loaded\n",
      "6bf7 Loaded\n",
      "WARNING:tensorflow:From /Users/markroza/Documents/work_from_home/NNcourse_project/v_env_p37/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/markroza/Documents/GitHub/work_from_home/NNproject/code/jupyter/../python/dataset_loader.py:176: batch_and_drop_remainder (from tensorflow.contrib.data.python.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.batch(..., drop_remainder=True)`.\n"
     ]
    }
   ],
   "source": [
    "#define folders\n",
    "base_data_folder = \"/Users/markroza/Documents/work_from_home/NNcourse_project/data/\"\n",
    "data_fld = base_data_folder + \"/res6/synth_exp/\"\n",
    "out_fld = base_data_folder + \"/results/disc_exp/\"\n",
    "\n",
    "\n",
    "model_path = out_fld+'/network_test/'\n",
    "graph_folder = out_fld+'/graphs/'\n",
    "test_res_folder = out_fld + '/tests/'\n",
    "\n",
    "if os.path.isdir(out_fld):\n",
    "    shutil.rmtree(out_fld, ignore_errors=True)\n",
    "\n",
    "os.mkdir(out_fld)\n",
    "os.mkdir(model_path)\n",
    "os.mkdir(graph_folder)\n",
    "os.mkdir(test_res_folder)\n",
    "\n",
    "\n",
    "\n",
    "syn_data_pairs = dataset_loader.read_list_file(data_fld+'list_synth.txt')\n",
    "syn_pdbs = [x[0] for x in  syn_data_pairs]\n",
    "real_data_pairs = dataset_loader.read_list_file(data_fld+'list_real.txt')\n",
    "real_pdbs = [x[0] for x in  real_data_pairs]\n",
    "\n",
    "all_data = EM_DATA_REAL_SYTH(data_fld,real_pdbs = real_pdbs[:4],synth_pdbs =syn_pdbs[:4], is_random = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/markroza/Documents/work_from_home/NNcourse_project/v_env_p37/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /Users/markroza/Documents/GitHub/work_from_home/NNproject/code/jupyter/../python/net_3d_1.py:69: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From /Users/markroza/Documents/work_from_home/NNcourse_project/v_env_p37/lib/python3.7/site-packages/tensorflow/python/ops/losses/losses_impl.py:209: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(net_3d_1)\n",
    "#tf.reset_default_graph() \n",
    "nn = net_3d_1.DISC_V1()\n",
    "all_iter = all_data.train_dataset.make_initializable_iterator()\n",
    "all_pair = all_iter.get_next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D Loss  0.8113421201705933 Disc Acc 0.8113421201705933 \n",
      "D Loss  0.6854454278945923 Disc Acc 0.6854454278945923 \n",
      "D Loss  0.6810461282730103 Disc Acc 0.6810461282730103 \n",
      "D Loss  0.680732011795044 Disc Acc 0.680732011795044 \n",
      "D Loss  0.6863856315612793 Disc Acc 0.6863856315612793 \n",
      "D Loss  0.6841870546340942 Disc Acc 0.6841870546340942 \n",
      "D Loss  0.6819883584976196 Disc Acc 0.6819883584976196 \n",
      "D Loss  0.684501051902771 Disc Acc 0.684501051902771 \n",
      "D Loss  0.681360125541687 Disc Acc 0.681360125541687 \n",
      "D Loss  0.6876420378684998 Disc Acc 0.6876420378684998 \n",
      "D Loss  0.686071515083313 Disc Acc 0.686071515083313 \n",
      "D Loss  0.6838729381561279 Disc Acc 0.6838729381561279 \n",
      "D Loss  0.6832448244094849 Disc Acc 0.6832448244094849 \n",
      "D Loss  0.6885842680931091 Disc Acc 0.6885842680931091 \n",
      "D Loss  0.6832447052001953 Disc Acc 0.6832447052001953 \n",
      "D Loss  0.6779052019119263 Disc Acc 0.6779052019119263 \n",
      "D Loss  0.6829306483268738 Disc Acc 0.6829306483268738 \n",
      "D Loss  0.6832448244094849 Disc Acc 0.6832448244094849 \n",
      "D Loss  0.6832447052001953 Disc Acc 0.6832447052001953 \n",
      "D Loss  0.6845011115074158 Disc Acc 0.6845011115074158 \n",
      "D Loss  0.6823024749755859 Disc Acc 0.6823024749755859 \n",
      "D Loss  0.6832448244094849 Disc Acc 0.6832448244094849 \n",
      "D Loss  0.6860715746879578 Disc Acc 0.6860715746879578 \n",
      "D Loss  0.6804179549217224 Disc Acc 0.6804179549217224 \n",
      "D Loss  0.6829307079315186 Disc Acc 0.6829307079315186 \n",
      "D Loss  0.6829306483268738 Disc Acc 0.6829306483268738 \n",
      "D Loss  0.6819883584976196 Disc Acc 0.6819883584976196 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-693657ee0b98>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mN_batches\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mmaps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_pair\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mp_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_points\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m55900\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work_from_home/NNcourse_project/v_env_p37/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work_from_home/NNcourse_project/v_env_p37/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work_from_home/NNcourse_project/v_env_p37/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work_from_home/NNcourse_project/v_env_p37/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work_from_home/NNcourse_project/v_env_p37/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1317\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1319\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/work_from_home/NNcourse_project/v_env_p37/lib/python3.7/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1405\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# open session and initialize all variables\n",
    "config = tf.ConfigProto(log_device_placement=False)\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    #sess = tf.InteractiveSession()\n",
    "    tf.global_variables_initializer().run()\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "    time_start = timer()\n",
    "    sess.run(all_iter.initializer)\n",
    "    # training-loop\n",
    "\n",
    "    for batch in range(all_data.N_batches-1):\n",
    "            maps, labels = sess.run(all_pair)\n",
    "            \n",
    "            p_real = all_data.train_points[55900]\n",
    "            p_synth = all_data.train_points[1655900]\n",
    "            \n",
    "            map_real = dataset_loader.get_out_map(p_real)\n",
    "            map_synth = dataset_loader.get_out_map(p_synth)\n",
    "            \n",
    "            label_r = dataset_loader.get_real_synth(p_real)\n",
    "            label_s = dataset_loader.get_real_synth(p_synth)\n",
    "            \n",
    "\n",
    "            \n",
    "            for i in range(dataset_loader.BATCH_SIZE):\n",
    "                if np.random.random()>0.5:\n",
    "                    maps[i,:,:,:,:] = 10\n",
    "                    labels[i,:,:,:,:] = 1#label_r\n",
    "                else:\n",
    "                    maps[i,:,:,:,:] = map_synth\n",
    "                    labels[i,:,:,:,:] =  0 #label_s\n",
    "            \n",
    "            feed_dict={nn.x: maps, nn.x_label: labels, nn.keep_prob: 0.8, nn.isTrain: True}\n",
    "\n",
    "            loss_d, acc_d, _= sess.run([nn.disc_loss[\"loss\"], nn.disc_loss[\"acc\"],nn.D_optim],\\\n",
    "                                       feed_dict={nn.x: maps, nn.x_label: labels, nn.keep_prob: 0.8, nn.isTrain: True})\n",
    "\n",
    "            if batch %100 == 0: \n",
    "            #sess.run(nn.clip)\n",
    "                print(\"D Loss  {} Disc Acc {} \".format(loss_d,acc_d))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "            p_real = all_data.train_points[55900]\n",
    "            p_synth = all_data.train_points[1655900]\n",
    "            \n",
    "            map_real = dataset_loader.get_out_map(p_real)\n",
    "            map_synth = dataset_loader.get_out_map(p_synth)\n",
    "            \n",
    "            label_r = dataset_loader.get_real_synth(p_real)\n",
    "            label_s = dataset_loader.get_real_synth(p_synth)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nn.isTrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, bins, patches = plt.hist(x=np.reshape(map_patch,-1),bins=10,range=[-2.0,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.std(map_patch),np.mean(map_patch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p37_tf",
   "language": "python",
   "name": "p37_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

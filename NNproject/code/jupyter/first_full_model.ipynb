{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize all imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import importlib\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "if '__file__' in locals():\n",
    "    dir_path = os.path.dirname(os.path.realpath(__file__))\n",
    "else:\n",
    "    dir_path = os.getcwd()\n",
    "python_path = dir_path + '/../python/'\n",
    "sys.path.append(python_path)\n",
    "\n",
    "\n",
    "import dataset_loader\n",
    "importlib.reload(dataset_loader)\n",
    "from dataset_loader import EM_DATA\n",
    "from dataset_loader import BATCH_SIZE, NBOX_IN,NBOX_OUT,N_CHANNELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eager Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "#tf.enable_eager_execution()\n",
    "print(tf.executing_eagerly())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Folder and Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "fld = \"/Users/markroza/Documents/work_from_home/NNcourse_project/data/first_tests/single_pdbs/\"\n",
    "outfld = fld+\"../single_pdbs_out/\"\n",
    "if os.path.isdir(outfld) is False:\n",
    "    os.mkdir(outfld)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/markroza/Documents/work_from_home/NNcourse_project/v_env_p37/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "WARNING:tensorflow:From /Users/markroza/Documents/work_from_home/NNcourse_project/v_env_p37/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "em1 = EM_DATA(fld,train_pdbs=['hhhh','nnnn'],test_pdbs=['oooo','cccc','ssss'])\n",
    "if not tf.executing_eagerly():\n",
    "    train_data = em1.train_dataset.make_initializable_iterator()\n",
    "    test_data = em1.test_dataset.make_initializable_iterator()\n",
    "    trn = train_data.get_next()\n",
    "    tst = test_data.get_next()\n",
    "else:\n",
    "    x = np.random.random([dataset_loader.BATCH_SIZE]+em1.feature_shape)\n",
    "    y = np.random.random([dataset_loader.BATCH_SIZE]+em1.label_shape)\n",
    "    trn = (x,y)\n",
    "    tst= (x,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Databaseses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "(2, 9, 9, 9, 5)\n",
      "TEST\n",
      "TRAIN\n",
      "(2, 9, 9, 9, 5)\n",
      "TEST\n",
      "TRAIN\n",
      "(2, 9, 9, 9, 5)\n",
      "TEST\n",
      "TRAIN\n",
      "(2, 9, 9, 9, 5)\n",
      "TEST\n",
      "TRAIN\n",
      "(2, 9, 9, 9, 5)\n",
      "TEST\n",
      "TRAIN\n",
      "(2, 9, 9, 9, 5)\n",
      "TEST\n",
      "TRAIN\n",
      "(2, 9, 9, 9, 5)\n",
      "TEST\n",
      "TRAIN\n",
      "(2, 9, 9, 9, 5)\n",
      "TEST\n",
      "TRAIN\n",
      "(2, 9, 9, 9, 5)\n",
      "TEST\n",
      "TRAIN\n",
      "(2, 9, 9, 9, 5)\n",
      "TEST\n"
     ]
    }
   ],
   "source": [
    "if not tf.executing_eagerly():\n",
    "    with tf.Session() as sess:\n",
    "            sess.run(train_data.initializer)\n",
    "            sess.run(test_data.initializer)\n",
    "            for k in range(10):\n",
    "                print('TRAIN')\n",
    "                x,y = sess.run(trn)\n",
    "                print(x.shape)\n",
    "                #print(np.sign([np.sum(x[0,:,:,:]),np.sum(x[1,:,:,:]),np.sum(x[2,:,:,:]),np.sum(x[3,:,:,:]),np.sum(x[4,:,:,:]),np.sum(y[:,:,:])]))\n",
    "                print('TEST')\n",
    "                x,y = sess.run(tst)\n",
    "                #print(np.sign([np.sum(x[0,:,:,:]),np.sum(x[1,:,:,:]),np.sum(x[2,:,:,:]),np.sum(x[3,:,:,:]),np.sum(x[4,:,:,:]),np.sum(y[:,:,:])]))\n",
    "else:\n",
    "    print(\"EAGER EXECUTION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'net_3d_1' from '/Users/markroza/Documents/GitHub/work_from_home/NNproject/code/jupyter/../python/net_3d_1.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import net_3d_1\n",
    "importlib.reload(net_3d_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From /Users/markroza/Documents/GitHub/work_from_home/NNproject/code/jupyter/../python/net_3d_1.py:43: conv3d (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv3d instead.\n",
      "WARNING:tensorflow:From /Users/markroza/Documents/GitHub/work_from_home/NNproject/code/jupyter/../python/net_3d_1.py:50: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n",
      "WARNING:tensorflow:From /Users/markroza/Documents/GitHub/work_from_home/NNproject/code/jupyter/../python/net_3d_1.py:70: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/markroza/Documents/work_from_home/NNcourse_project/v_env_p37/lib/python3.7/site-packages/tensorflow/contrib/layers/python/layers/layers.py:1624: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:From /Users/markroza/Documents/GitHub/work_from_home/NNproject/code/jupyter/../python/net_3d_1.py:72: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From /Users/markroza/Documents/GitHub/work_from_home/NNproject/code/jupyter/../python/net_3d_1.py:93: conv3d_transpose (from tensorflow.python.layers.convolutional) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.conv3d_transpose instead.\n",
      "WARNING:tensorflow:From /Users/markroza/Documents/work_from_home/NNcourse_project/v_env_p37/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "nn = net_3d_1.get_net_graph(trn[0],trn[1],0.9,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN\n",
      "(2, 9, 9, 9, 5)\n",
      "(2, 5, 5, 5, 1)\n",
      "URA\n",
      "encoder l1 shape (2, 9, 9, 9, 128)\n",
      "encoder l2 shape (2, 5, 5, 5, 256)\n",
      "encoder l3 shape (2, 3, 3, 3, 512)\n",
      "encoder l4 shape (2, 1, 1, 1, 100)\n",
      "encoder z shape (2, 100)\n",
      "generator l1 shape (2, 2, 2, 2, 512)\n",
      "generator l2 shape (2, 3, 3, 3, 256)\n",
      "generator l3 shape (2, 5, 5, 5, 128)\n",
      "generator c4 shape (2, 5, 5, 5, 1)\n",
      "generator output shape (2, 5, 5, 5, 1)\n",
      "discriminator l1 shape (2, 5, 5, 5, 128)\n",
      "discriminator l2 shape (2, 3, 3, 3, 256)\n",
      "discriminator l3 shape (2, 2, 2, 2, 512)\n",
      "discriminator c4 shape (2, 1, 1, 1, 2)\n",
      "discriminator output shape (2, 1, 1, 1, 2)\n",
      "discriminator logits shape (2, 1, 1, 1, 2)\n",
      "reconstruction_loss shape (2,)\n",
      "KL divergence shape (2,)\n",
      "VAE loss  shape ()\n",
      "VAE_Loss value  87.967545\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "        tf.global_variables_initializer().run()\n",
    "\n",
    "        sess.run(train_data.initializer)\n",
    "        sess.run(test_data.initializer)\n",
    "        for k in range(1):\n",
    "            print('TRAIN')\n",
    "            x,y = sess.run(trn)\n",
    "            print(x.shape)\n",
    "            b = sess.run(nn[\"x_3D\"], feed_dict={nn[\"x_3D\"]:y})\n",
    "            print(b.shape)\n",
    "            print(\"URA\")\n",
    "            enc_l1 = sess.run(nn[\"enc\"][\"l1\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"encoder l1 shape\",enc_l1.shape)\n",
    "            enc_l2 = sess.run(nn[\"enc\"][\"l2\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"encoder l2 shape\",enc_l2.shape)\n",
    "            enc_l3 = sess.run(nn[\"enc\"][\"l3\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"encoder l3 shape\",enc_l3.shape)\n",
    "            enc_l4 = sess.run(nn[\"enc\"][\"l4\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"encoder l4 shape\",enc_l4.shape)\n",
    "            enc_z = sess.run(nn[\"enc\"][\"z\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"encoder z shape\",enc_z.shape)\n",
    "            gen_l1 = sess.run(nn[\"G_z\"][\"l1\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"generator l1 shape\",gen_l1.shape)\n",
    "            gen_l2 = sess.run(nn[\"G_z\"][\"l2\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"generator l2 shape\",gen_l2.shape)\n",
    "            gen_l3 = sess.run(nn[\"G_z\"][\"l3\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"generator l3 shape\",gen_l3.shape)\n",
    "            gen_c4 = sess.run(nn[\"G_z\"][\"c4\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"generator c4 shape\",gen_c4.shape)\n",
    "            gen_o = sess.run(nn[\"G_z\"][\"o\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"generator output shape\",gen_o.shape)\n",
    "            \n",
    "            dsc_l1 = sess.run(nn[\"dsc_real\"][\"l1\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"discriminator l1 shape\",dsc_l1.shape)\n",
    "            dsc_l2 = sess.run(nn[\"dsc_real\"][\"l2\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"discriminator l2 shape\",dsc_l2.shape)\n",
    "            dsc_l3 = sess.run(nn[\"dsc_real\"][\"l3\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"discriminator l3 shape\",dsc_l3.shape)\n",
    "            dsc_c4 = sess.run(nn[\"dsc_real\"][\"c4\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"discriminator c4 shape\",dsc_c4.shape)\n",
    "            dsc_o = sess.run(nn[\"dsc_real\"][\"o\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"discriminator output shape\",dsc_o.shape)\n",
    "            dsc_logits = sess.run(nn[\"dsc_real\"][\"logits\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"discriminator logits shape\",dsc_logits.shape)\n",
    "\n",
    "            rec_loss = sess.run(nn[\"R_L\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"reconstruction_loss shape\",rec_loss.shape)\n",
    "            KL_div = sess.run(nn[\"KL_D\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"KL divergence shape\",KL_div.shape)\n",
    "            VAE_loss = sess.run(nn[\"VAE_L\"])#, feed_dict={nn[\"x_image\"]:x,nn[\"x_3D\"]:y,nn[\"keep_prob\"]: 0.8, nn[\"isTrain\"]: True})\n",
    "            print(\"VAE loss  shape\",VAE_loss.shape)\n",
    "            print(\"VAE_Loss value \", VAE_loss)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare New Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "\n",
    "\n",
    "em1 = EM_DATA(fld,box_size= 8,train_pdbs=['hhhh','nnnn'],test_pdbs=['oooo','cccc','ssss'])\n",
    "train_data = em1.train_dataset.make_initializable_iterator()\n",
    "test_data = em1.test_dataset.make_initializable_iterator()\n",
    "trn = train_data.get_next()\n",
    "tst = test_data.get_next()\n",
    "\n",
    "\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[BATCH_SIZE, NBOX_IN, NBOX_IN, NBOX_IN,N_CHANNELS ])\n",
    "y = tf.placeholder(tf.float32, shape=[BATCH_SIZE, NBOX_OUT, NBOX_OUT, NBOX_OUT,1 ])\n",
    "keep_prob = tf.placeholder(dtype=tf.float32)\n",
    "isTrain = tf.placeholder(dtype=tf.bool)\n",
    "\n",
    "nn = net_3d_1.get_net_graph(x,y,keep_prob,isTrain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 0\n",
      "D Loss: 0.08211641\n",
      "G Loss: 0.08484885\n",
      "VAE loss: 22.937546\n",
      "KL divergence: 9.166757\n",
      "reconstruction_loss: 82.625626\n",
      "###########\n",
      "batch: 0\n",
      "D Loss: 0.63054824\n",
      "G Loss: 0.45340738\n",
      "VAE loss: 293.82635\n",
      "KL divergence: 117.52106\n",
      "reconstruction_loss: 94.9436\n",
      "###########\n",
      "batch: 0\n",
      "D Loss: -0.073429726\n",
      "G Loss: -0.080447145\n",
      "VAE loss: 15.242507\n",
      "KL divergence: 6.0883293\n",
      "reconstruction_loss: 86.733696\n",
      "###########\n",
      "batch: 0\n",
      "D Loss: -0.38368452\n",
      "G Loss: -0.3552776\n",
      "VAE loss: 375.3949\n",
      "KL divergence: 150.14867\n",
      "reconstruction_loss: 92.8924\n",
      "###########\n",
      "batch: 1\n",
      "D Loss: -0.28760675\n",
      "G Loss: -0.30389643\n",
      "VAE loss: 337.79462\n",
      "KL divergence: 135.10948\n",
      "reconstruction_loss: 83.61467\n",
      "###########\n",
      "batch: 1\n",
      "D Loss: 0.15908688\n",
      "G Loss: 0.16054754\n",
      "VAE loss: 53.35824\n",
      "KL divergence: 21.333939\n",
      "reconstruction_loss: 93.55952\n",
      "###########\n",
      "batch: 1\n",
      "D Loss: -0.02546097\n",
      "G Loss: 0.003516227\n",
      "VAE loss: 395.2951\n",
      "KL divergence: 158.10938\n",
      "reconstruction_loss: 86.626396\n",
      "###########\n",
      "batch: 1\n",
      "D Loss: -0.084424265\n",
      "G Loss: 0.027387805\n",
      "VAE loss: 61.27857\n",
      "KL divergence: 24.502102\n",
      "reconstruction_loss: 93.24153\n",
      "###########\n",
      "batch: 2\n",
      "D Loss: 0.1711487\n",
      "G Loss: 0.26906762\n",
      "VAE loss: 333.77295\n",
      "KL divergence: 133.49994\n",
      "reconstruction_loss: 92.299934\n",
      "###########\n",
      "batch: 2\n",
      "D Loss: -4.23193e-06\n",
      "G Loss: -0.32077554\n",
      "VAE loss: 1060.1245\n",
      "KL divergence: 424.0402\n",
      "reconstruction_loss: 96.23971\n",
      "###########\n",
      "batch: 2\n",
      "D Loss: -0.3442515\n",
      "G Loss: -0.34510612\n",
      "VAE loss: 39.258896\n",
      "KL divergence: 15.694106\n",
      "reconstruction_loss: 94.51866\n",
      "###########\n",
      "batch: 2\n",
      "D Loss: 0.112107754\n",
      "G Loss: 0.045353327\n",
      "VAE loss: 560.7523\n",
      "KL divergence: 224.2924\n",
      "reconstruction_loss: 85.18538\n",
      "###########\n",
      "batch: 3\n",
      "D Loss: -0.18679377\n",
      "G Loss: -0.17984745\n",
      "VAE loss: 254.77484\n",
      "KL divergence: 101.9012\n",
      "reconstruction_loss: 87.33529\n",
      "###########\n",
      "batch: 3\n",
      "D Loss: 0.14156365\n",
      "G Loss: -0.085857466\n",
      "VAE loss: 232.05103\n",
      "KL divergence: 92.811134\n",
      "reconstruction_loss: 92.76868\n",
      "###########\n",
      "batch: 3\n",
      "D Loss: -0.96940315\n",
      "G Loss: -0.710551\n",
      "VAE loss: 76.81387\n",
      "KL divergence: 30.71637\n",
      "reconstruction_loss: 91.78401\n",
      "###########\n",
      "batch: 3\n",
      "D Loss: 0.085122705\n",
      "G Loss: 0.071299784\n",
      "VAE loss: 118.37955\n",
      "KL divergence: 47.343132\n",
      "reconstruction_loss: 86.87918\n",
      "###########\n",
      "batch: 4\n",
      "D Loss: 0.54142267\n",
      "G Loss: 0.40738752\n",
      "VAE loss: 1047.693\n",
      "KL divergence: 419.06854\n",
      "reconstruction_loss: 86.72301\n",
      "###########\n",
      "batch: 4\n",
      "D Loss: -0.17402792\n",
      "G Loss: -0.25331742\n",
      "VAE loss: 254.52493\n",
      "KL divergence: 101.80014\n",
      "reconstruction_loss: 98.4292\n",
      "###########\n",
      "batch: 4\n",
      "D Loss: 0.19139703\n",
      "G Loss: 0.18804927\n",
      "VAE loss: 69.3909\n",
      "KL divergence: 27.747082\n",
      "reconstruction_loss: 92.79104\n",
      "###########\n",
      "batch: 4\n",
      "D Loss: -0.24116103\n",
      "G Loss: -0.2526142\n",
      "VAE loss: 20.815552\n",
      "KL divergence: 8.31742\n",
      "reconstruction_loss: 88.01727\n",
      "###########\n",
      "batch: 5\n",
      "D Loss: -0.019471262\n",
      "G Loss: -0.021993704\n",
      "VAE loss: 226.78381\n",
      "KL divergence: 90.70584\n",
      "reconstruction_loss: 76.81406\n",
      "###########\n",
      "batch: 5\n",
      "D Loss: 0.01575532\n",
      "G Loss: 0.019445613\n",
      "VAE loss: 347.98242\n",
      "KL divergence: 139.18362\n",
      "reconstruction_loss: 93.382675\n",
      "###########\n",
      "batch: 5\n",
      "D Loss: 0.8072115\n",
      "G Loss: 0.80180776\n",
      "VAE loss: 1459.9775\n",
      "KL divergence: 583.9825\n",
      "reconstruction_loss: 85.30704\n",
      "###########\n",
      "batch: 5\n",
      "D Loss: -0.02690561\n",
      "G Loss: -0.046734214\n",
      "VAE loss: 818.72\n",
      "KL divergence: 327.4788\n",
      "reconstruction_loss: 92.121414\n",
      "###########\n",
      "batch: 6\n",
      "D Loss: 0.37451115\n",
      "G Loss: 0.37158534\n",
      "VAE loss: 211.99146\n",
      "KL divergence: 84.78839\n",
      "reconstruction_loss: 81.91784\n",
      "###########\n",
      "batch: 6\n",
      "D Loss: 0.52442694\n",
      "G Loss: 0.45155174\n",
      "VAE loss: 236.23846\n",
      "KL divergence: 94.48636\n",
      "reconstruction_loss: 90.27508\n",
      "###########\n",
      "batch: 6\n",
      "D Loss: 0.69398534\n",
      "G Loss: 0.69549346\n",
      "VAE loss: 363.06537\n",
      "KL divergence: 145.21756\n",
      "reconstruction_loss: 85.79146\n",
      "###########\n",
      "batch: 6\n",
      "D Loss: -0.5923696\n",
      "G Loss: -0.6017324\n",
      "VAE loss: 30.090872\n",
      "KL divergence: 12.02751\n",
      "reconstruction_loss: 88.3891\n",
      "###########\n",
      "batch: 7\n",
      "D Loss: 0.90250164\n",
      "G Loss: 0.43808505\n",
      "VAE loss: 88.87497\n",
      "KL divergence: 35.541046\n",
      "reconstruction_loss: 89.4171\n",
      "###########\n",
      "batch: 7\n",
      "D Loss: 0.010870588\n",
      "G Loss: 0.010434508\n",
      "VAE loss: 20.505886\n",
      "KL divergence: 8.193732\n",
      "reconstruction_loss: 86.223564\n",
      "###########\n",
      "batch: 7\n",
      "D Loss: 1.6835818\n",
      "G Loss: 1.6760534\n",
      "VAE loss: 271.00394\n",
      "KL divergence: 108.39183\n",
      "reconstruction_loss: 97.48118\n",
      "###########\n",
      "batch: 7\n",
      "D Loss: 1.9215977\n",
      "G Loss: 1.1317488\n",
      "VAE loss: 821.23474\n",
      "KL divergence: 328.4848\n",
      "reconstruction_loss: 91.07085\n",
      "###########\n",
      "batch: 8\n",
      "D Loss: 0.6864539\n",
      "G Loss: 0.64020205\n",
      "VAE loss: 29.940548\n",
      "KL divergence: 11.9666815\n",
      "reconstruction_loss: 95.37482\n",
      "###########\n",
      "batch: 8\n",
      "D Loss: 0.27876574\n",
      "G Loss: 0.08792396\n",
      "VAE loss: 62.265377\n",
      "KL divergence: 24.896568\n",
      "reconstruction_loss: 95.80972\n",
      "###########\n",
      "batch: 8\n",
      "D Loss: 0.46103668\n",
      "G Loss: 0.27896178\n",
      "VAE loss: 173.51212\n",
      "KL divergence: 69.39595\n",
      "reconstruction_loss: 88.98956\n",
      "###########\n",
      "batch: 8\n",
      "D Loss: 0.76961845\n",
      "G Loss: 0.666485\n",
      "VAE loss: 40.058235\n",
      "KL divergence: 16.014256\n",
      "reconstruction_loss: 90.37891\n",
      "###########\n",
      "batch: 9\n",
      "D Loss: 1.303338\n",
      "G Loss: 1.2591901\n",
      "VAE loss: 26.660954\n",
      "KL divergence: 10.655049\n",
      "reconstruction_loss: 93.31477\n",
      "###########\n",
      "batch: 9\n",
      "D Loss: -1.826272\n",
      "G Loss: -1.7600194\n",
      "VAE loss: 356.07877\n",
      "KL divergence: 142.42288\n",
      "reconstruction_loss: 86.26358\n",
      "###########\n",
      "batch: 9\n",
      "D Loss: 0.3710821\n",
      "G Loss: 0.34342486\n",
      "VAE loss: 37.891228\n",
      "KL divergence: 15.14674\n",
      "reconstruction_loss: 97.51013\n",
      "###########\n",
      "batch: 9\n",
      "D Loss: 0.80847836\n",
      "G Loss: 0.14203575\n",
      "VAE loss: 351.13504\n",
      "KL divergence: 140.44368\n",
      "reconstruction_loss: 103.389984\n",
      "###########\n",
      "batch: 10\n",
      "D Loss: -0.0622242\n",
      "G Loss: -0.001674369\n",
      "VAE loss: 70.12786\n",
      "KL divergence: 28.041218\n",
      "reconstruction_loss: 99.26573\n",
      "###########\n",
      "batch: 10\n",
      "D Loss: 1.2048752\n",
      "G Loss: 1.0617375\n",
      "VAE loss: 2163.0576\n",
      "KL divergence: 865.2144\n",
      "reconstruction_loss: 86.371414\n",
      "###########\n",
      "batch: 10\n",
      "D Loss: -0.19731857\n",
      "G Loss: -0.21765666\n",
      "VAE loss: 62.673027\n",
      "KL divergence: 25.05986\n",
      "reconstruction_loss: 93.5174\n",
      "###########\n",
      "batch: 10\n",
      "D Loss: -0.40773666\n",
      "G Loss: -0.92595345\n",
      "VAE loss: 981.6366\n",
      "KL divergence: 392.64612\n",
      "reconstruction_loss: 85.40806\n",
      "###########\n",
      "batch: 11\n",
      "D Loss: 1.3253158\n",
      "G Loss: 0.1625108\n",
      "VAE loss: 163.19206\n",
      "KL divergence: 65.268\n",
      "reconstruction_loss: 88.30269\n",
      "###########\n",
      "batch: 11\n",
      "D Loss: 0.55873615\n",
      "G Loss: 0.5457335\n",
      "VAE loss: 505.34946\n",
      "KL divergence: 202.13011\n",
      "reconstruction_loss: 96.69986\n",
      "###########\n",
      "batch: 11\n",
      "D Loss: -0.78559375\n",
      "G Loss: -0.8453013\n",
      "VAE loss: 488.536\n",
      "KL divergence: 195.4056\n",
      "reconstruction_loss: 88.107635\n",
      "###########\n",
      "batch: 11\n",
      "D Loss: 1.8617048\n",
      "G Loss: 1.809349\n",
      "VAE loss: 35.182365\n",
      "KL divergence: 14.064093\n",
      "reconstruction_loss: 88.52528\n",
      "###########\n",
      "batch: 12\n",
      "D Loss: -1.2516829\n",
      "G Loss: -1.2122986\n",
      "VAE loss: 523.6199\n",
      "KL divergence: 209.43898\n",
      "reconstruction_loss: 89.60089\n",
      "###########\n",
      "batch: 12\n",
      "D Loss: 4.289651\n",
      "G Loss: 2.0495622\n",
      "VAE loss: 73.2484\n",
      "KL divergence: 29.28993\n",
      "reconstruction_loss: 94.2948\n",
      "###########\n",
      "batch: 12\n",
      "D Loss: 0.56914276\n",
      "G Loss: 0.6000613\n",
      "VAE loss: 239.00113\n",
      "KL divergence: 95.591774\n",
      "reconstruction_loss: 86.78873\n",
      "###########\n",
      "batch: 12\n",
      "D Loss: -0.38921598\n",
      "G Loss: -0.4023763\n",
      "VAE loss: 18.909924\n",
      "KL divergence: 7.5543146\n",
      "reconstruction_loss: 96.54116\n",
      "###########\n",
      "batch: 13\n",
      "D Loss: 1.2643473\n",
      "G Loss: 1.2399236\n",
      "VAE loss: 387.35754\n",
      "KL divergence: 154.93433\n",
      "reconstruction_loss: 86.89514\n",
      "###########\n",
      "batch: 13\n",
      "D Loss: 1.9715266\n",
      "G Loss: 1.9633603\n",
      "VAE loss: 288.35544\n",
      "KL divergence: 115.33229\n",
      "reconstruction_loss: 98.901855\n",
      "###########\n",
      "batch: 13\n",
      "D Loss: 1.2426922\n",
      "G Loss: 0.40645692\n",
      "VAE loss: 1031.7456\n",
      "KL divergence: 412.6886\n",
      "reconstruction_loss: 96.64877\n",
      "###########\n",
      "batch: 13\n",
      "D Loss: -0.7048067\n",
      "G Loss: -0.27068394\n",
      "VAE loss: 653.418\n",
      "KL divergence: 261.35754\n",
      "reconstruction_loss: 96.52968\n",
      "###########\n",
      "batch: 14\n",
      "D Loss: 2.816376\n",
      "G Loss: 2.7702909\n",
      "VAE loss: 69.956566\n",
      "KL divergence: 27.973217\n",
      "reconstruction_loss: 94.11391\n",
      "###########\n",
      "batch: 14\n",
      "D Loss: 1.9004632\n",
      "G Loss: 1.879312\n",
      "VAE loss: 18.274998\n",
      "KL divergence: 7.300046\n",
      "reconstruction_loss: 99.52881\n",
      "###########\n",
      "batch: 14\n",
      "D Loss: -2.6879742\n",
      "G Loss: -2.3811069\n",
      "VAE loss: 686.9429\n",
      "KL divergence: 274.76764\n",
      "reconstruction_loss: 95.126434\n",
      "###########\n",
      "batch: 14\n",
      "D Loss: 0.051059112\n",
      "G Loss: 0.23125702\n",
      "VAE loss: 63.101433\n",
      "KL divergence: 25.230112\n",
      "reconstruction_loss: 104.59598\n",
      "###########\n",
      "batch: 15\n",
      "D Loss: 0.51826763\n",
      "G Loss: -0.22672784\n",
      "VAE loss: 137.67227\n",
      "KL divergence: 55.059322\n",
      "reconstruction_loss: 95.89758\n",
      "###########\n",
      "batch: 15\n",
      "D Loss: 1.665345\n",
      "G Loss: 1.6626766\n",
      "VAE loss: 39.75953\n",
      "KL divergence: 15.894388\n",
      "reconstruction_loss: 94.22122\n",
      "###########\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch: 15\n",
      "D Loss: 1.3551826\n",
      "G Loss: -0.5900234\n",
      "VAE loss: 1342.5498\n",
      "KL divergence: 537.01\n",
      "reconstruction_loss: 99.09111\n",
      "###########\n",
      "batch: 15\n",
      "D Loss: 0.7466397\n",
      "G Loss: 0.7110299\n",
      "VAE loss: 698.02673\n",
      "KL divergence: 279.20084\n",
      "reconstruction_loss: 98.66872\n",
      "###########\n",
      "batch: 16\n",
      "D Loss: -1.8786082\n",
      "G Loss: -2.0907109\n",
      "VAE loss: 324.7553\n",
      "KL divergence: 129.89153\n",
      "reconstruction_loss: 105.93995\n",
      "###########\n",
      "batch: 16\n",
      "D Loss: -0.78264356\n",
      "G Loss: -0.7228057\n",
      "VAE loss: 487.32135\n",
      "KL divergence: 194.918\n",
      "reconstruction_loss: 105.30545\n",
      "###########\n",
      "batch: 16\n",
      "D Loss: 1.6931803\n",
      "G Loss: 0.58924\n",
      "VAE loss: 588.8727\n",
      "KL divergence: 235.53908\n",
      "reconstruction_loss: 100.04864\n",
      "###########\n",
      "batch: 16\n",
      "D Loss: -1.8828543\n",
      "G Loss: -1.9137592\n",
      "VAE loss: 163.99094\n",
      "KL divergence: 65.58544\n",
      "reconstruction_loss: 109.24037\n",
      "###########\n",
      "batch: 17\n",
      "D Loss: 2.388229\n",
      "G Loss: 2.9009066\n",
      "VAE loss: 89.25073\n",
      "KL divergence: 35.689423\n",
      "reconstruction_loss: 108.713776\n",
      "###########\n",
      "batch: 17\n",
      "D Loss: 2.0404167\n",
      "G Loss: 1.642556\n",
      "VAE loss: 496.4579\n",
      "KL divergence: 198.57275\n",
      "reconstruction_loss: 104.02938\n",
      "###########\n",
      "batch: 17\n",
      "D Loss: 2.3448467\n",
      "G Loss: 1.9172888\n",
      "VAE loss: 213.96472\n",
      "KL divergence: 85.57631\n",
      "reconstruction_loss: 95.80131\n",
      "###########\n",
      "batch: 17\n",
      "D Loss: -1.8866649\n",
      "G Loss: -1.8883383\n",
      "VAE loss: 434.71094\n",
      "KL divergence: 173.8745\n",
      "reconstruction_loss: 98.8029\n",
      "###########\n",
      "batch: 18\n",
      "D Loss: -0.23479992\n",
      "G Loss: 0.67120755\n",
      "VAE loss: 182.5376\n",
      "KL divergence: 73.0044\n",
      "reconstruction_loss: 106.42293\n",
      "###########\n",
      "batch: 18\n",
      "D Loss: 1.1467391\n",
      "G Loss: -0.050905466\n",
      "VAE loss: 452.6161\n",
      "KL divergence: 181.0357\n",
      "reconstruction_loss: 107.376724\n",
      "###########\n",
      "batch: 18\n",
      "D Loss: 3.221924\n",
      "G Loss: 2.6492748\n",
      "VAE loss: 75.927475\n",
      "KL divergence: 30.359842\n",
      "reconstruction_loss: 111.48616\n",
      "###########\n",
      "batch: 18\n",
      "D Loss: 0.4552729\n",
      "G Loss: 0.154898\n",
      "VAE loss: 36.25942\n",
      "KL divergence: 14.494003\n",
      "reconstruction_loss: 97.64456\n",
      "###########\n",
      "batch: 19\n",
      "D Loss: 1.4386454\n",
      "G Loss: 1.3957014\n",
      "VAE loss: 239.35202\n",
      "KL divergence: 95.73055\n",
      "reconstruction_loss: 102.59004\n",
      "###########\n",
      "batch: 19\n",
      "D Loss: -1.1885719\n",
      "G Loss: -1.1214209\n",
      "VAE loss: 42.602684\n",
      "KL divergence: 17.030987\n",
      "reconstruction_loss: 100.86842\n",
      "###########\n",
      "batch: 19\n",
      "D Loss: -1.0752919\n",
      "G Loss: -1.1161532\n",
      "VAE loss: 14.093117\n",
      "KL divergence: 5.627415\n",
      "reconstruction_loss: 98.31399\n",
      "###########\n",
      "batch: 19\n",
      "D Loss: 0.8783435\n",
      "G Loss: 1.1083577\n",
      "VAE loss: 42.30816\n",
      "KL divergence: 16.913113\n",
      "reconstruction_loss: 101.505356\n",
      "###########\n"
     ]
    }
   ],
   "source": [
    "# open session and initialize all variables\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    #sess = tf.InteractiveSession()\n",
    "    tf.global_variables_initializer().run()\n",
    "    sess.run(train_data.initializer)\n",
    "\n",
    "    logger = tf.summary.FileWriter('./graphs', sess.graph)\n",
    "    merged = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "    model_path = './network_test_chair/'\n",
    "    if os.path.isdir(model_path) is False:\n",
    "        os.mkdir(model_path)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "        # training-loop\n",
    "    for batch in range(20):\n",
    "        for _ in range(4):\n",
    "            image_in,image_out = sess.run(trn)\n",
    "            sess.run(nn[\"D_optim\"], feed_dict={x: image_in, y: image_out, keep_prob: 0.8, isTrain: True})\n",
    "            sess.run(nn[\"clip\"])\n",
    "            loss_d_, loss_g_, _VAE_loss, _KL_divergence, _reconstruction_loss, summary,\\\n",
    "            _, _, _ = sess.run([nn[\"D_loss\"], nn[\"G_loss\"], nn[\"VAE_L\"], nn[\"mean_KL\"],\\\n",
    "            nn[\"mean_recon\"], merged, nn[\"D_optim\"], nn[\"G_optim\"], nn[\"E_optim\"]],\\\n",
    "                        feed_dict={x: image_in, y: image_out, keep_prob: 0.8, isTrain: True})\n",
    "            sess.run(nn[\"clip\"])\n",
    "            if batch % 1 == 0:\n",
    "                print(\"batch:\", batch)\n",
    "                print(\"D Loss:\", loss_d_)\n",
    "                print(\"G Loss:\", loss_g_)\n",
    "                print(\"VAE loss:\", _VAE_loss)\n",
    "                print(\"KL divergence:\", _KL_divergence)\n",
    "                print(\"reconstruction_loss:\", _reconstruction_loss)\n",
    "                print(\"###########\")\n",
    "                if batch % 5 == 0:\n",
    "                    G = sess.run(nn[\"G_z\"][\"o\"], feed_dict={x: image_in, y: image_out, keep_prob: 0.8, isTrain: False})\n",
    "                    np.save(outfld + str(batch) + \".npy\", G)\n",
    "                    if batch % 10 == 0:\n",
    "                        saver.save(sess, model_path + str(batch) + \".ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p37_tf",
   "language": "python",
   "name": "p37_tf"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
